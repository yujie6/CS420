
\chapter{March 16}
\section{逻辑回归} 
概率判别模型在分类时往往更加可靠，因为如果用函数的话就无法求导了。
\subsection{二分类}
\noindent \textbf{损失函数}
用交叉熵来衡量
\[
		\mathcal{L}\left( y,x \right)  = -\sum_{k} \delta\left( y=c_{k} \right) \log p_{\theta}\left( y=c_k \mid x \right) 
.\] 
其中
\[
		\delta\left( z \right)  = \begin{cases}
				1, \quad z \text{ is true} \\
				0, \quad z \text{ else}
		\end{cases}
.\] 

为了输出一个$\left( 0,1 \right) $ 之间的值，我们会给$z=\theta^{T}x$套上一个sigmoid函数
\[
		\sigma\left( z \right)  = \frac{1}{1+\exp\left( -z \right) }
.\] 
即
\[
		p\left( y=1 \mid x \right) =\sigma\left( z \right), \quad p\left( y=0 \mid x \right) = 1-\sigma\left( z \right) 
.\] 
带入损失函数有
\[
		\mathcal{L}\left( y,x \right)  = -y\log \sigma\left( z \right)  - \left( 1-y \right) \log\left( 1-\sigma\left( z \right)  \right) 
.\] 
有趣的是，$\sigma$ 求导后
\[
		\frac{\partial \sigma\left( z \right) }{\partial z} = \sigma\left( z \right) \left( 1-\sigma\left( z \right)  \right) 
.\] 
从而梯度为
\[
		\frac{\partial \mathcal{L}\left( y,x \right) }{\partial \theta}  = \left( \sigma\left( z \right) -y \right) x
.\] 
可以看到，梯度形式与线性回归十分相似。
\subsection{多分类}
与二分类同样的，只是引入一个类别集
\[
C = \{c_1,c_2,\ldots,c_{n}\} 
.\] 
使用softmax
\[
		p_{\theta}\left( x=c_{j}  \mid  x \right)  = \frac{\exp\left( \theta_{j}^{T}x \right) }{\sum_{i=1}^{m} \exp\left( \theta_{i^{T}x} \right) }
.\] 
参数$\theta = \{\theta_{1},\ldots,\theta_{m}\} $, 可以同时除掉$\theta_{1}$ 来减少一个参数。

其实可以看成$m$ 个二分类，目标
\[
		\mathrm{max} \log p_{\theta}\left( y=c_{j} \mid x \right) 
.\] 
求梯度有
\begin{align*}
		\frac{\partial \log p_{\theta}}{\partial \theta_{j}} 
		&= x - p_{\theta}x  \\
		&= \left( 1-p_{\theta}\left( y=c_j \mid x \right)  \right) x 
.\end{align*} 
\subsection{逻辑回归应用}
\noindent \textbf{在线广告点击率(CTR)估算}
Feature engineering十分重要
\begin{itemize}
		\item One-Hot二进制编码 
        \item 例如\verb|weekday=friday| 转化为一个七维向量。容易发现转化的高维向量极为稀疏。		
\end{itemize}

\section{支持向量机} 
线性分类器本质上是划一个决策边界，但是考虑到数据噪声后，分类器很容易出错。
我们发现边界与数据距离最大时是最
robust的，也称之为最大间隔边界。

我们的优化目标便是最短距离最大化。逻辑回归的打分函数$s=\theta^{T} x$，容易
发现如果打分越高的样例越远离决策边界，分类器越可靠。

\subsection{SVM最优化问题}
假设
\begin{itemize}
		\item 标签$y \in \{-1,1\} $ 
		\item $h\left( x \right) = g\left( w^{T}x + b \right) $ \footnote{g = sgn}
\end{itemize}
由高中知识我们知道，点到直线的距离即几何间隔为
\[
		\gamma^{(i)} = \frac{ \lvert w^{T}x^{(i)} + b   \rvert }{\|w\|^2}
.\]
不妨设$\|w\|^2 = 1$，且由于$y^{(i)}$ 的取值只为$\{-1,1\} $，上式可简化为
\begin{equation}	
		\gamma^{(i)} = y^{(i)} \left( w^{T}x^{(i)} + b \right) \label{eqn:1}
.\end{equation}
 
除此之外还可以通过简单的代数手段进行推导：
\[
		w^{T} \left( x^{(i)} - \gamma^{(i)} y^{(i)} \frac{w}{\|w\|} \right)  + b= 0
.\] 
解得
\begin{equation}
		\gamma^{(i)} = y^{(i)} \left[ \left( \frac{w}{\|w\|} \right) ^{T} x^{(i)} + \frac{b}{\|w\|} \right] \label{eqn:2}
.\end{equation}
容易发现\eqref{eqn:1}和\eqref{eqn:2}本质相同，从而最小几何间隔为
\[
		\hat{ \gamma} = \mathrm{min}_{i} \left( \gamma^{(1)},\ldots, \gamma^{(m)} \right) 
.\] 
等价于在
\[
		y^{(i)} \left( w^{T}x^{(i)}  + b \right) \ge  \hat{\gamma}
.\] 
时求$\max \gamma$，但是非凸目标函数不易求最值，所以需要进行转换。
将函数间隔固定为1，即$\hat{\gamma} = 1$，目标函数转化为
\[
\min \frac{1}{2} \|w \|^2
.\] 
使得$y^{(i)} \left( w^{T}x^{(i)} + b \right)  \ge  1$

\section{支持向量机优化} 
\subsection{拉格朗日对偶问题}
对于凸优化问题，可以使用拉格朗日乘数法。但是只能处理限制条件是等式的
情况，为了处理不等式，我们需要引入松弛变量将其转化为等式，例如
\[
		g_{i}\left( w \right)  = 1 - y^{(i)} \left( w^{T}x^{(i)} + b \right)  \le  0
.\] 
$g_{i}\left( w \right) $ 即支持向量。
这等价于存在$a_{i}$使得
\[
		g_{i}\left( w \right)  + a_{i}^2 = 0
.\] 
得到拉格朗日函数
\[
		\mathcal{L} \left( w,\lambda,a \right)  = 
		f\left( w \right)  + \sum_{i=1}^{n} \lambda_{i} \left( g_{i}\left( w \right) + 
		a_{i}^2 \right) 
.\] 
其中$f$ 即优化目标$\frac{1}{2} \|w\|^2 $由拉格朗日乘数法知
\[
\begin{cases}
		\frac{\partial \mathcal{L}}{\partial w} = 0 \\ 
		\lambda_{i} a_{i} = 0 \\
		g_{i} \left( w \right)  + a_{i} ^2 = 0 \\ 
		\lambda_{i} \ge 0
\end{cases}
.\] 
对$\lambda_{i}a_{i} = 0$讨论，若$a_{i}=0$则$g_{i}\left( w \right) =0$，
若$a_{i}\neq 0$ 则必有$\lambda_{i} = 0$。

知$a_{i}$ 这个变量对我们求最值没有任何约束，我们可以用$\lambda_{i}g_{i}\left( w \right) 
=0$ 来代替上式的条件。删去$a_{i}$ 即可得到\href{https://zhuanlan.zhihu.com/p/77750026}{KKT条件}，

从而拉格朗日函数可以重写为
\[
		\mathcal{L}\left( w,\lambda \right) = f(w) + \sum_{i=1}^{n} \lambda_{i}g_{i}\left( w \right) 
.\] 
注意到$\mathcal{L} \le  f\left( w \right) $ ，当调整$\lambda$使得
$\sum_{i=1}^{n} \lambda_{i}g_{i}\left( w \right) $大的时候，
$f\left( w \right) $一定会更小，故我们的最优化问题等价于
\[
		\min_{w} \max_{\lambda} \mathcal{L} \left( w, \lambda \right) 
.\] 
可以证明KKT条件下有强对偶性
\[
\min \max f = \max \min f
.\] 
%下面考虑取等的情况，即$$
在svm中$\lambda  = 0$ 是没用的点，$\lambda > 0$ 则说明$g_{i}=0$ 即这个点刚好在最优边界上。
\subsection{支持向量机优化求解} 
由强对偶性，可以很容易求解之前的问题，先对$w$求偏导，可以知道问题转化为
$
\sum_{i=1}^{m} \lambda_{i} y^{i} = 0
$
时求解
\[
\max_{\lambda} \left[ \sum_{i=1}^{m} \lambda_{i} + 
\frac{1}{2}\lambda_{i}\lambda_{j} y^{(i)} y^{(j)} \left( (x^{(i)})^{T}  x^{(j)} \right)    \right] 
.\] 
可以使用序列最小优化(Sequential Minimization Optimization) 来求解
求解到$\lambda$之后可以直接求解$w,b$. 

\noindent \textbf{线性不可分情况}：\\
应用场景中，数据往往\textbf{线性不可分}。在SVM中我们一般加入松弛变量$\xi$ (但需要注意的是
引入松弛变量只能解决噪声问题，如果真的不可分则需采用核方法)
\[
		y^{(i)}\left( w^{T}x^{(i)} + b \right) \ge 1 - \xi_{i}, \quad \xi_{1} \ge  0
.\] 
而拉格朗日函数也会相应的改变。我们要求$\lambda_{i} \le  C$.
\noindent \textbf{损失函数对比}:\\
我们称SVM的损失函数为Hinge Loss:
\[
		\frac{1}{2} \|w\|^2 + C \sum_{i=1}^{m} \max\left( 0, 1- y^{(i)}\left( w^{T}x^{(i)}+b \right)  \right) 
.\] 
它与logistic loss的区别在于只要支持向量大于一了，就没有相关的loss了。
\subsection{SMO算法}
SMO的思想也叫坐标上升法（Coordinate Ascent），与梯度下降的区别在于$SMO$每次优化会固定部分坐标，只优化一个维度。
考虑到$\lambda_i$之间是互相有约束的
\[
\sum_{i=1}^{m} \lambda_{i} y^{i} = 0
.\] 
我们选取两个变量$\lambda_i, \lambda_j$进行更新，对$W\left( \lambda \right) $ 进行优化。
例如$i=1,j=2$。其他变量为常数，我们的约束变为
\[
\lambda_1 y^{(i)} + \lambda_2 y^{(2)} =-\sum_{i=3}^{m} \lambda_{i}y^{(i)} = \zeta  
.\] 

最后的优化变成一个二次函数
 \[
		 W\left( \lambda_2 \right)  = a \lambda_2^2 + b \lambda + c  
.\] 
